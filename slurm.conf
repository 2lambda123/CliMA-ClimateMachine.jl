ClusterName=central
SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
StateSaveLocation=/central/slurm/state
SlurmdSpoolDir=/var/spool/slurmd
SwitchType=switch/none
#MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
ReturnToService=2
MaxJobCount=100000
TaskPlugin=affinity,cgroup

# TIMERS
# HA Timer:
SlurmctldTimeout=300
MessageTimeout=45

# SCHEDULING
PriorityType=priority/multifactor
PriorityDecayHalfLife=14-0
PriorityWeightFairshare=10000
# changing partition weight down since there is only 1 and it is showing a larger weight that seems appropriate.
PriorityWeightPartition=100
PriorityWeightJobSize=100
PriorityWeightAge=100
PriorityWeightqos=10000
PriorityMaxAge=7-0

#PriorityWeightqos=1000000


# SUSPEND AND RESUME SCRIPTS FOR CLOUD BRUSTING

SuspendProgram=/central/slurm/scripts/suspend.py
ResumeProgram=/central/slurm/scripts/resume.py
ResumeFailProgram=/central/slurm/scripts/suspend.py

SuspendTimeout=600
ResumeTimeout=600
ResumeRate=0
SuspendRate=0
SuspendTime=300

# Cloud specifc parameters:

SlurmctldParameters=idle_on_node_suspend

#TreeWidth=65533
TreeWidth=500

SuspendExcNodes=hpc-22-[07-24,28,30,32,34,36,38],hpc-23-[07-24,28,30,32,34,36,38],hpc-24-[07-24,28,30,32,34,36,38],hpc-25-[03-10,14-15,17-18,20-21,23-24],hpc-26-[14-15,17-18,20-21,23-24],hpc-80-[04-09,11-16,18-23,25-28,33,36],hpc-81-[04-09,11-16,18-23,25-27,33,35,37],hpc-82-[04-09,11-16,18-23,25-27,33,35,37],hpc-83-[04-09,11-16,18-23,25-27,33,35,37],hpc-89-[03-26,32-33,35-38],hpc-90-[03-26,29-30,32-33,35-38],hpc-91-[09-21,24-25,28-30,32-33],hpc-92-[03-26,29-30,32-33,36-38],hpc-93-[03-26,29-30,32-33,36-39]

#PrivateData=cloud

DebugFlags=PowerSave,NO_CONF_HASH

# adding prioty for gpu jobs.

PriorityWeightTRES=CPU=100,GRES/gpu=30000

# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurmctld
SlurmdDebug=info
SlurmdLogFile=/var/log/slurmd

JobCompType=jobcomp/filetxt
JobCompLoc=/central/groups/esm/lenka/ClimateMachine.jl/job_comp.log

#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherType=jobacct_gather/cgroup
#JobAcctGatherFrequency=30
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
# Can add additional types like gres/gpu:p100 if desired
AccountingStorageTRES=gres/gpu
AccountingStorageEnforce=qos,safe

# Scheduler
SchedulerType=sched/backfill
SchedulerParameters=bf_window=20160,bf_resolution=600,bf_continue,bf_max_job_test=400
#SchedulerParameters=bf_window=20160,bf_resolution=600,bf_continue,bf_max_job_test=400
JobSubmitPlugins=require_timelimit
PrologFlags=contain,x11

# change to cons_tres to give more options on gpus.  09/03/2020
#SelectType=select/cons_res
SelectType=select/cons_tres

SelectTypeParameters=CR_Core_Memory
#LaunchParameters=send_gids
RequeueExit=240
#increased to 180 November 28 2018 due to nodes draining
UnkillableStepTimeout=180
RebootProgram=/usr/sbin/reboot

# added after working with schedmd - bug 5913
# they can be removed in 18.08 or above
#MemLimitEnforce=no
#JobAcctGatherParams=NoOverMemoryKill


# Master nodes
ControlMachine=head1
BackupController=head2
AccountingStorageHost=head1
AccountingStorageBackupHost=head2

# Generic resources types
GresTypes=gpu

MpiDefault=pmi2


Licenses=spartan:8


#NodeName=DEFAULT CPUs=32 SocketsPerBoard=2 CoresPerSocket=16

# Nodes GPU

NodeName=hpc-22-[28,30,32,34,36,38],hpc-23-[28,30,32,34,36,38],hpc-24-[28,30,32,34,36,38],hpc-25-[14-15,17-18,20-21,23-24],hpc-26-[14-15,17-18,20-21,23-24],hpc-89-[35-36,38],hpc-90-[35-38],hpc-91-[32-33],hpc-92-[36-38],hpc-93-[36-38] Gres=gpu:p100:4 CPUs=28 CoresPerSocket=14 RealMemory=250000 Features=broadwell weight=30

# Nodes GPU with V100

#Nodename=hpc-89-37 Gres=gpu:p100:2 Gres=gpu:v100:2 CPUs=28 CoresPerSocket=14 RealMemory=250000 Features=broadwell weight=50
Nodename=hpc-89-37 Gres=gpu:p100:2,gpu:v100:2 CPUs=28 CoresPerSocket=14 RealMemory=250000 Features=broadwell weight=50

# Nodes Highmem Skylake

NodeName=hpc-89-[32-33],hpc-90-[32-33],hpc-91-[28-30],hpc-92-[32-33],hpc-93-[32-33] RealMemory=380000 CPUs=32 CoresPerSocket=16 Features=skylake weight=15

# Nodes Compute Skylake

NodeName=hpc-22-[07-24],hpc-23-[07-24],hpc-24-[07-24],hpc-25-[03-10],hpc-89-[03-26],hpc-90-[03-26,29-30],hpc-91-[09-21,24-25],hpc-92-[03-26,29-30],hpc-93-[03-26,29-30] RealMemory=188000 CPUs=32 CoresPerSocket=16 Features=skylake weight=10


# Nodes Highmem  CascadeLake

NodeName=hpc-81-[33,35,37],hpc-82-[33,35,37],hpc-83-[33,35,37]  CPUs=56  ThreadsPerCore=1 RealMemory=1540000 Features=cascadelake weight=60


# Nodes Highmem HighCore CascadeLake

NodeName=hpc-80-[33,36]  CPUs=96  CoresPerSocket=24 ThreadsPerCore=1 RealMemory=1540000 Features=cascadelake weight=65

# Nodes Compute CascadeLake

NodeName=hpc-80-[04-09,11-16,18-23,25-28],hpc-81-[04-09,11-16,18-23,25-27],hpc-82-[04-09,11-16,18-23,25-27],hpc-83-[04-09,11-16,18-23,25-27] CPUs=56 CoresPerSocket=28  RealMemory=380000 Features=cascadelake weight=20

# Nodes DGX1

NodeName=hpc-93-39 Gres=gpu:v100:8 CPUs=80 Boards=1 SocketsPerBoard=2 CoresPerSocket=20 ThreadsPerCore=2 RealMemory=1030000


# Nodes Cloud

#NodeName=clima-custom-2-12800-20-0-[1-20] RealMemory=9600 CPUS=2 State=CLOUD
#NodeName=clima-n1-standard-8-20-0-[1-10] CPUs=8 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=2 RealMemory=29995  State=CLOUD
#NodeName=clima-custom-2-12800-20-0-[1-20] State=CLOUD 
#NodeName=clima-n1-standard-8-20-0-[1-10] State=CLOUD 


# these need to be indexed to the partitions in /central/slurm/scripts/config.yaml

#NodeName=clima-0-[1-250]  CPUs=8 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=2 RealMemory=29995  State=CLOUD
#NodeName=clima-1-[1-250]  CPUs=8 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=2 RealMemory=29995  State=CLOUD
#NodeName=clima-2-[1-250]  CPUs=16 Boards=1 SocketsPerBoard=1 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=59990  State=CLOUD
#NodeName=clima-3-[1-250]  CPUs=16 Boards=1 SocketsPerBoard=1 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=59990  State=CLOUD

NodeName=clima-0-[1-250] Gres=gpu:v100:1 CPUs=8 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=2 RealMemory=29995  State=CLOUD
NodeName=clima-1-[1-250] Gres=gpu:v100:2 CPUs=8 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=2 RealMemory=29995  State=CLOUD
NodeName=clima-2-[1-250] Gres=gpu:v100:3 CPUs=16 Boards=1 SocketsPerBoard=1 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=59990  State=CLOUD
NodeName=clima-3-[1-250] Gres=gpu:v100:4 CPUs=16 Boards=1 SocketsPerBoard=1 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=59990  State=CLOUD
NodeName=ipac-4-[1-10] CPUs=8 Boards=1 SocketsPerBoard=1 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=29995  State=CLOUD


# Partitions
PartitionName=DEFAULT DefMemPerCPU=4096

###  Partition ANY

# original partition.  will need to switch to this once foreman testing is comlete
#PartitionName=any Default=YES Nodes=hpc-22-[07-24,28,30,32,34,36,38],hpc-23-[07-24,28,30,32,34,36,38],hpc-24-[07-24,28,30,32,34,36,38],hpc-25-[03-10,14-15,17-18,20-21,23-24],hpc-26-[14-15,17-18,20-21,23-24],hpc-89-[03-26,32-33,35-38],hpc-90-[03-26,29-30,32-33,35-38],hpc-91-[09-21,24-25,28-30,32-33],hpc-92-[03-26,29-30,32-33,36-38],hpc-93-[03-26,29-30,32-33,36-38] MaxTime=14-0 DenyAccounts=sunshine

# any parition during foreman testing
#PartitionName=any Default=YES Nodes=hpc-22-[07-24,28,30,32,34,36,38],hpc-23-[07-24,28,30,32,34,36,38],hpc-24-[07-24,28,30,32,34,36,38],hpc-25-[03-10,14-15,17-18,20-21,23-24],hpc-26-[14-15,17-18,20-21,23-24],hpc-89-[03-26,32-33,35-38],hpc-90-[03-26,29-30,32-33,35-38],hpc-91-[09-21,24-25,28-30,32-33],hpc-92-[03-26,29-30,32-33,36-38],hpc-93-[03-20] MaxTime=14-0 DenyAccounts=sunshine

# any parition during tensorlab testing, will need to switch back to original when parition is gone
#PartitionName=any Default=YES Nodes=hpc-22-[07-24,28,30,32,34,36,38],hpc-23-[07-24,28,30,32,34,36,38],hpc-24-[07-24,28,30,32,34,36,38],hpc-25-[03-10,14-15,18,20-21,23-24],hpc-26-[14-15,17-18,20-21,23-24],hpc-89-[03-26,32-33,35-38],hpc-90-[03-26,29-30,32-33,35-38],hpc-91-[09-21,24-25,28-30,32-33],hpc-92-[03-26,29-30,32-33,36-38],hpc-93-[03-26,29-30,32-33,36-38] MaxTime=14-0 DenyAccounts=sunshine


# any parition during tensorlab testing, will need to switch back to original when parition is gone
PartitionName=any Default=YES Nodes=hpc-22-[07-24,28,30,32,34,36,38],hpc-23-[07-24,28,30,32,34,36,38],hpc-24-[07-24,28,30,32,34,36,38],hpc-25-[03-10,14-15,18,20-21,23-24],hpc-26-[14-15,17-18,20-21,23-24],hpc-80-[04-09,11-16,18-23,25-28,33,36],hpc-81-[04-09,11-16,18-23,25-27,33,35,37],hpc-82-[04-09,11-16,18-23,25-27,33,35,37],hpc-83-[04-09,11-16,18-23,25-27,33,35,37],hpc-89-[03-26,32-33,35-38],hpc-90-[03-26,29-30,32-33,35-38],hpc-91-[09-21,24-25,28-30,32-33],hpc-92-[03-26,29-30,32-33,36-38],hpc-93-[03-26,29-30,32-33,36-38] MaxTime=14-0 DenyAccounts=sunshine

# Partition for Tom Miller's DGX

PartitionName=sunshine Default=NO Nodes=hpc-93-39 PriorityTier=100 AllowAccounts=sunshine

PartitionName=dgxlo Default=NO Nodes=hpc-93-39 PriorityTier=1 AllowAccounts=tensorlab
#PartitionName=dgxlo Default=NO Nodes=hpc-93-39 PriorityTier=100 AllowAccounts=tensorlab

# Test partition for anima

PartitionName=tensortest Default=NO Nodes=hpc-25-17 PriorityTier=1 AllowAccounts=tensorlab

# Partition for Foreman testing

#PartitionName=test Default=NO Nodes=hpc-80-[04-09,11-16,18-23,25-28,33,36],hpc-81-[04-09,11-16,18-23,25-27,33,35,37],hpc-82-[04-09,11-16,18-23,25-27,33,35,37],hpc-83-[04-09,11-16,18-23,25-27,33,35,37]

#  Partition for Google Cloud

#PartitionName=clima Default=NO Nodes=clima-custom-2-12800-20-0-[1-20]  MaxTime=INFINITE State=UP LLN=yes
#PartitionName=clima2 Default=NO Nodes=clima-n1-standard-8-20-0-[1-10]  MaxTime=INFINITE State=UP LLN=yes

PartitionName=clima Default=NO Nodes=clima-0-[1-250]  MaxTime=INFINITE State=UP LLN=yes
PartitionName=clima2 Default=NO Nodes=clima-1-[1-250]  MaxTime=INFINITE State=UP LLN=yes
PartitionName=clima3 Default=NO Nodes=clima-2-[1-250]  MaxTime=INFINITE State=UP LLN=yes
PartitionName=clima4 Default=NO Nodes=clima-3-[1-250]  MaxTime=INFINITE State=UP LLN=yes
PartitionName=ipac Default=NO Nodes=ipac-4-[1-10]  MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE LLN=yes



# commented this out, thought it was already removed  November 28, 2018 - naveed
#PartitionName=spartan Nodes=hpc-90-[17-24] MaxTime=14-0

###  PartitionName=any Default=YES Nodes=ALL MaxTime=14-0

#PartitionName=compute Default=YES Nodes=hpc-22-[07-24],hpc-23-[07-24],hpc-24-[07-24],hpc-25-[03-10],hpc-89-[03-26],hpc-90-[03-26,29-30],hpc-91-[09-21,24-25],hpc-92-[03-26,29-30],hpc-93-[03-26,29-30]

#PartitionName=highmem Nodes=hpc-89-[32-33],hpc-90-[32-33],hpc-91-[28-30],hpc-92-[32-33],hpc-93-[32-33]

#PartitionName=gpu Nodes=hpc-22-[28,30,32,34,36,38],hpc-23-[28,30,32,34,36,38],hpc-24-[28,30,32,34,36,38],hpc-25-[14-15,17-18,20-21,23-24],hpc-26-[14-15,17-18,20-21,23-24],hpc-89-[35-38],hpc-90-[35-38],hpc-91-[32-33],hpc-92-[36-38],hpc-93-[36-38]


###

#include /central/slurm/etc/cloud.conf
